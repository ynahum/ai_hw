\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{comment}

\graphicspath{ {./} }

\newcommand{\rectres}[1]{
\begin{center}
\begin{tabular}{ |c| }
\hline\\
#1\\
\\
\hline
\end{tabular}
\end{center}
}

\newcommand{\qed}{\hfill$\blacksquare$}

\title{Introduction to AI - 236501\\HW3}
\author{Yair Nahum 034462796\\and\\Hala Awwad 209419134 }

\begin{document}

\maketitle

%\tableofcontents{}

\section*{MDP}

\subsection*{A}

\subsubsection*{A.1.a}

In order to work with our R(s) instead of R(s,a,s') one can define the following:
$$\tilde R(s_t=s) = \mathbb {E}_{s'\in S, a\in A(s)}[R(s_{t+1}=s',a_t=a|s_t=s]=$$
$$= \sum_{s'\in S} \sum_{a\in A(s)} P(s_{t+1}=s',a_t=a|s_t=s) R(s_t=s,a_t=a,s_{t+1}=s') = $$
$$= \sum_{s'\in S} \sum_{a\in A(s)} P(s_{t+1}=s'|a_t=a,s_t=s)\pi(a_t=a|s_t=s) R(s_t=s,a_t=a,s_{t+1}=s') = $$
$$= \sum_{a\in A(s)} \pi(a_t=a|s_t=s) \sum_{s'\in S} P(s_{t+1}=s'|a_t=a,s_t=s) R(s_t=s,a_t=a,s_{t+1}=s') = $$

Thus, the value function on states $s\in S$ with some given policy $\pi(a|s)$ (probability to make an action $a$ when we're in s) is defined recursively by:

$$V^{\pi}(s) = \sum_{a\in A(s)} \pi(a|s) \sum_{s'\in S} P(s'|s,a) [R(s,a,s') + \gamma V(s')]$$

In case we have a deterministic given policy, we can write it as:

$$V^{\pi}(s) = \sum_{s'\in S} P(s'|s,\pi(s)) [R(s,\pi(s),s') + \gamma V(s')]$$

(We assumed a stationary policy and dynamics so we didn't subscript/relate to time on probabilities.)

\subsubsection*{A.1.b}

The Bellman operator:

$$V^*(s) = \max_{a\in A(s)}Q^*(s,a) = \max_{a\in A(s)} \sum_{s'\in S} P(s'|s,a) [R(s,a,s') + \gamma V^*(s')]$$

\subsubsection*{A.1.c}

Value Iteration (VI):\\

\begin{enumerate}
  \item Start with any initial value function (zero or random) $V_0(s),\quad \forall s\in S$.
  \item Compute recursively, for $n = 0,1,2, \ldots $ umtil stopping rule is met (see next the stopping rule details:
          \[V_{n + 1}(s) = \max_{a\in A(s)} \sum_{s' \in S} P(s'|s,a)[R(s,a,s') + \gamma V_n(s')], \quad \forall s \in S\]
   \item Extract optimal policy from optimal value by:
   \[\pi(s) \in arg\max_{a\in A(s)} \sum_{s' \in S} P(s'|s,a)[R(s,a,s') + \gamma V^*(s')], \quad \forall s \in S\]
\end{enumerate}
Notes:\\
1. Stopping rule to obtain a required accuracy.\\
  If $$||V_{n+1}-V_{n}||_\infty < \epsilon \cdot \frac{1-\gamma}{\gamma}$$ then $$||V_{n+1} -
V^{*}||_\infty \leq \epsilon$$\\
(this is a theorem that can be proved as done in tutorials)\\
2. For a fixed policy value iteration, the recursive computation is changed to have $\pi(s)$ instead of $a$
$$V^{\pi}_{n + 1}(s) = \sum_{s' \in S} P(s'|s,\pi(s))[R(s,\pi(s),s') + \gamma V^{\pi}_n(s')], \quad \forall s \in S$$
This can be solved also by linear system of equations solver as these are linear equations (usually when the state space is not too big)\\

When $\gamma=1$ VI is not guaranteed to converge unless there are absorbing/goal states ($s\in S_G$ no action to transition out from it) that we can get to with probability 1 eventually ( This is known as episodic MDPs or Stochastic Shortest Path problems).

\subsubsection*{A.1.d}

Policy Iteration (PI):\\
PI consists of 2 main parts: Policy evaluation and Policy improvement.

\begin{enumerate}
\item Select some stationary policy $\pi_0$.
\item For $k = 0,1,2, \ldots $ until stopping rule is met (see next the stopping rule details:
\begin{enumerate}
\item Policy Evaluation: compute $V^{\pi_k}$ by fixed policy VI or by solving linear system of equations:\\
$$V^{\pi_k}_{n + 1}(s) = \sum_{s' \in S} P(s'|s,\pi_k(s))[R(s,\pi_k(s),s') + \gamma V^{\pi_k}_n(s')], \quad \forall s \in S$$
Or:\\
$$V^{\pi_k} = (I - \gamma P^{\pi_k})^{-1} P^{\pi_k}R^{\pi_k}$$
(at the last closed solution there is the $R^{\pi_k}$ which corresponds to the vector of rewards $R(s,\pi_k(s),s')$ multiplied by the transition probabilities matrix for expectancy)
\item Policy Improvement: compute $\pi_{k+1}$, a greedy policy with respect to $V^{\pi_k}$:
$$\pi_{k+1}(s) \in arg\max_{a\in A(s)} \sum_{s' \in S} P(s'|s,a)[R(s,a,s') + \gamma V^{\pi_k}(s')], \quad \forall s \in S$$
\item Stop if $\pi_{k+1} = \pi_{k}$
\end{enumerate}
\end{enumerate}

When $\gamma=1$ PI is not guaranteed to converge unless there are absorbing/goal states ($s\in S_G$ no action to transition out from it) that we can get to with probability 1 eventually ( This is known as episodic MDPs or Stochastic Shortest Path problems).

\subsubsection*{A.1.e}

\includegraphics[]{hw3/plots/A_5.PNG}

\subsubsection*{A.1.f}

\includegraphics[]{hw3/plots/A_6.PNG}

\subsection*{B}
Wet part code intro.

\subsection*{C}
Wet part in code.

\subsection*{Bonus}
We've implemented the code and used the previous section parts in order to calculate optimal policies. in specific the policy iteration:\\
\\
\textbf{from value\_and\_policy\_iteration import policy\_iteration}\\
\newpage\\
.\\
All optimal policies for the given rewards and $\gamma=1$:\\
\includegraphics[]{hw3/plots/all_optimal_policies.PNG}
\newpage\\
.\\
Reward ranges with specific optimal policy when $\gamma=0.9$:\\
\includegraphics[scale=0.9]{hw3/plots/r1.PNG}
\includegraphics[scale=0.9]{hw3/plots/r2.PNG}\\
\includegraphics[scale=0.9]{hw3/plots/r3.PNG}

\\
Note: In printing all reward ranges, since we used policy iteration which uses policy evaluation by inverting a matrix and $(I-\gamma P)^{-1}$ is not invert-able (0 eigen value by definition as P has and eigen value of 1) if $\gamma == 1$, printing all rewards ranges for $\gamma == 1$ case were not done.\\
We could have run value iteration each time and extract the optimal policy, but policy evaluation does it in less iterations.
So, we added some prints for $\gamma < 1$.\\

\section*{Learning Introduction}

\subsection*{A}
Wet part code intro.

\subsection*{B}
\subsubsection*{B.1}
\subsubsubsection{B.1.a.}
The entropy $H(Passed)$:
$$H(Passed)= -[\frac{2}{6}\log_2(\frac{2}{6}) + \frac{4}{6}\log_2\frac{4}{6}] = -[\frac{1}{3}(\log_21- \log_23) + \frac{2}{3}(\log_22 - \log_23)]=$$ 
$$-[\frac{2}{3} - \log_23] = \log_23 - \frac{2}{3} \Rightarrow$$
\rectres{$H(Passed)=\log_23 - \frac{2}{3} \approx 0.9183$}
\subsubsubsection{B.1.b.}
The entropy $H(Passed|Average)$:
$$H(Passed|Average=Low)= -[\frac{1}{2}\log_2\frac{1}{2} +\frac{1}{2}\log_2\frac{1}{2}] = 1$$ 
$$H(Passed|Average=Medium)= -[\frac{1}{2}\log_2\frac{1}{2} +\frac{1}{2}\log_2\frac{1}{2}] = 1$$ 
$$H(Passed|Average=High)= -[1\log_21 + 0\log_20] = 0$$ 
The weighted entropy:
$$H(Passed|Average)=\sum_{i=1}^3\frac{|E_i|}{|E|}entropy(E_i)=$$ $$=\frac{2}{6}H(Passed|Average=Low) + \frac{2}{6}H(Passed|Average=Medium)+$$
$$+\frac{2}{6}H(Passed|Average=High)=\frac{2}{3}\Rightarrow$$
\rectres{$H(Passed|Average)=\frac{2}{3}$}
\subsubsubsection{B.1.c.}
The entropy $H(Passed|Studied)$:
$$H(Passed|Studied=No)= -[\frac{2}{3}\log_2\frac{2}{3} +\frac{1}{3}\log_2\frac{1}{3}] = \log_23 - \frac{2}{3}$$ 
$$H(Passed|Studied=Yes)= -[1\log_21 + 0\log_20] = 0$$ 
The weighted entropy:
$$H(Passed|Studied)=\frac{3}{6}(\log_23 - \frac{2}{3}) + \frac{3}{6}0= \frac{1}{2}\log_23 - \frac{1}{3}$$ 
\rectres{$H(Passed|Studied)=\frac{1}{2}\log_23 - \frac{1}{3} \approx 0.4591$}
\newpage\\
.\\
\subsubsubsection{B.1.d.}\\
The IG is bigger when selecting the "studied" feature. so we get the following tree (based on ID3 algo with the given samples):\\
\includegraphics[]{hw3/plots/tree.PNG}

\subsubsection*{B.3}
\subsubsubsection{B.3.a.}\\
For $K=1$ we get 0 training error. since the nearest point to each example is itself.\\
\subsubsubsection{B.3.b.}\\
With too big K, for example $K>=5$, we will get that for $(y>x+7,x)$ half plane, we would predict wrongly as we don't have enough examples from the right prediction "+" and we will predict wrongly "-" (the same for $(y,x>y+7)$ half plane, we will predict wrongly "+").\\
This basically increases bias.\\
With too small K, for example $K=1,3$, we would get another problem as there are again many areas in which we don't have enough samples from the needed kind and thus the appearance of the other kind will dominate the prediction.\\
This is the problem of over-fit (high variance) due too not enough examples.\\
\subsubsubsection{B.3.c.}\\
For $K=1$, we predict wrongly on 10 out of 14 examples (the corners of the lines are predicted OK). Thus, for $K=1$ the LOOCV error is $\frac{10}{14}$.\\
For $K=3$, we predict wrongly on 6 out of 14 examples. Thus, for $K=3$ the LOOCV error is $\frac{6}{14}$.\\
For $K=5,7$, we predict wrongly on 4 out of 14 examples (the "+" above the "-" 5-line and the "-" below the "+" 5-line). Thus, for $K=5,7$ the LOOCV error is $\frac{4}{14}$.\\
For $K=9$ we get again $\frac{10}{14}$ as all the "-" 5-line and "+" 5-line are predicted wrongly. For other bigger K's, it is also very bad.\\
\\
Bottom line, for smallest LOOCV $\frac{2}{7}$, one should take $K=5$ or $K=7$\\
\\
\subsubsubsection{B.3.d.}\\
\includegraphics[scale=0.5]{hw3/plots/q3_answer.PNG}

\subsection*{C}
\subsubsection*{C.4}
In code. passed L2/accuracy unit tests.
\subsubsection*{C.5 - ID3 algorithm}
\subsubsubsection{C.5.a.}\\
In code.\\
\\
\subsubsubsection{C.5.b.}\\
basic experiment passed needed accuracy with 94.69\%

\subsubsection*{C.6 - early pruning}
\subsubsubsection{C.6.a.}\\
Pruning improves the generalization and prevent over-fitting. By having leaves with more labels to decide from, we increase our hypothesis statistical significance (law of big numbers) and the variance of our model is reduced.\\
\\
\subsubsubsection{C.6.b.}\\
In code.\\
\\
\subsubsubsection{C.6.c.}\\
\\
\subsubsubsection{C.6.c.i}\\
\includegraphics[scale=0.4]{hw3/plots/cv4.png}
\\
\newpage
The KFold CV run output:\\
\\
\begin{center}
\begin{tabular}{ ||c|c|| } 
 \hline
 M value & Validation Accuracy \\ 
 \hline
 25 & 90.40\% \\ 
 35 & 93.01\% \\ 
 45 & 90.95\% \\ 
 55 & 90.96\% \\ 
 65 & 90.09\% \\ 
 \hline
\end{tabular}
\end{center}
\begin{center}
\begin{tabular}{ ||c|c|| } 
 \hline
Best M  & Validation Accuracy \\ 
 \hline
 35 & 93.01\% \\ 
 \hline
\end{tabular}
\end{center}
\\
\subsubsubsection{C.6.c.ii}\\
According to the graph and output we can see that for minimum number of samples in a leaf $m=35$, we get the best accuracy 93.01\%.\\
We find this optimal value by K fold CV process.\\
\\
\subsubsubsection{C.6.d.}\\
running with the  $best\_m=35$ parameter we get better performance over the test data set as the accuracy is now 97.35\%



\end{document}

