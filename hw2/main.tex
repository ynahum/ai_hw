\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{comment}

\graphicspath{ {./} }

\newcommand{\rectres}[1]{
\begin{center}
\begin{tabular}{ |c| }
\hline\\
#1\\
\\
\hline
\end{tabular}
\end{center}
}

\newcommand{\qed}{\hfill$\blacksquare$}

\title{Introduction to AI - 236501\\HW2}
\author{Yair Nahum 034462796\\and\\Hala Awwad 209419134 }

\begin{document}

\maketitle

%\tableofcontents{}

\section*{A - Improved Greedy Agent}

\subsection*{A.1}

We will use the following parameters:\\
1. Agents' locations.\\
2. Passengers' locations.\\
3. Agents' cash.\\
4. Agents' fuel.\\
5. Gas stations locations.\\

Our agent difference in cash is playing a part in our value function heuristic as it gives us some indication
that our cash was increased (in case we drop a passenger in its destination) at this turn relative to
other operations we may take. We denote it by:
$$cash\_value = taxi.cash - other\_taxi.cash$$

Another important ingredient to our total value function is the fuel. Fuel worth money and the ability to get
more cash by serving passengers. It also plays a major part in our survivable in the game. If we don't refill
the fuel when it's close to empty, we may get stuck. We denote it by:
$$fuel\_value = taxi.fuel - other\_taxi.fuel$$

The Agents' locations and Passengers' locations play a major role in planning the agent's strategy and go towards almost assured cash gain vs the other agent's possibilities.
We calculate for each the possibilities:\\

if both passengers are waiting in the street we calculate the MD to the P0,P1 from each T0,T1. We also calculate the gain we can get after we drop the passenger. for example for T0->P0->D0:
$$taxi\_to\_dest\_value = 2 * MD(P0,D0)- (MD(T0,P0) + MD(P0,D0)) =$$
$$ MD(P0,D0)- MD(T0,P0)$$
We then compare the values from each taxi to each destination.
If we have both feasible before the other agent may reach the passenger, we select the one with the higher value. If only one of them is feasible before the other agent we go towards that passenger (we take that value).\\
If both are not feasible to take before the other taxi, we still select the one with the bigger value.\\

In case we already have a passenger in the taxi, the value is calculated as the cash at the end with the subtraction of the remaining cost:\\
$$taxi\_to\_dest\_value = 2 * MD(P0,D0)- (MD(T0,P0) + MD(P0,D0)) =$$
$$ 2 * MD(P0,D0)- (0 + MD(T0,D0)) = 2 * MD(P0,D0) - MD(T0,D0)$$

In case, we don't have a passenger but the other taxi does have, we calculate the value of getting the other passenger:
$$taxi\_to\_dest\_value = MD(P0,D0)- MD(T0,P0)$$

When the fuel gets below 7 (as the max MD on the board is 6), we add another value function that favors the gas stations. We calculate the MD towards each of it and get the max between the two.\\
We then calculate the value to survive and give it a weight of 10 as it's important not to get stuck w/o fuel:\\
$$max\_cost\_taxi\_to\_gas = max(cost\_taxi\_to\_gas0, cost\_taxi\_to\_gas1)$$
$$survival\_value = 10*(6 - max\_cost\_taxi\_to\_gas)$$

The total value , for each state we get to, is an accumulation of the above:
\rectres{$total\_value = survival\_value + taxi\_to\_dest\_value + cash\_value + fuel\_value$}


Important note, if we discover we can win in certain conditions the other agent, for example, the other taxi has no fuel and we have cash, or the other agent cannot gain enough cash to win our taxi, we don't add the survival value (filling fuel) and just keep wasting fuel until the game is done.

\subsection*{A.2}

The existing heuristic of the greedy agent is not performing well, as it gives for each next state the same value w/o considering the ability to gain more cash due to serving passengers.\\
The only state in which it directs us to the correct direction to gain cash is when it got a passenger and it arrives to its destination. In such case, it will drop it correctly and gain cash as the value increases compared to other next state values which have less cash gain.\\
We've explained in detail the formula and the motivation on the previous section.\\
We think and can see from running some tests that our improved greedy agent wins the original one almost always unless we hit corner cases (such as the other agent gets stuck with one passenger on the other passenger location or destination, thus we cannot take the remaining passenger)\\

\subsection*{A.3}
We've run games with max of 50 steps each.\\
We've run the following with random seeds increment between 0 to 50:\\
greedy vs greedy\_improved:\\
taxi greedy won 0/100 games!\\
taxi greedy\_improved won 76/100 games!\\
\\
greedy vs random:\\
taxi greedy won 0/100 games!\\
taxi random won 7/100 games!\\
\\
greedy\_improved vs random:\\
taxi greedy\_improved won 85/100 games!\\
taxi random won 1/100 games!\\
\\
We clearly see that the improved greedy wins much more.\\
There are corner cases in which it gets a draw such as when the other taxi agent gets one passenger and gets stuck w/o fuel on the other passenger destination.

\section*{B - Minimax Agent}

\subsection*{B.1}
As in DFS, we can get to a winning situation in a very deep and far from our current state as we go deep first and not as in BFS (to the breadth).\\
Thus, the agent can find a winning state (infinite value returned) but if we  continued to search in the next children of the current parent state, it would have found a faster (less deep) way to win the game.\\
There is no actual bug. However, to solve this issues, we can use iterative deepening as we did to DFS to make it admissible (finding the optimal path). This way, the agent will select the closest path to win the game.\\

\subsection*{B.2}
In iterative deepening we have 2 advantages:\\
1. When we need to stand in a specific time limit, we always return a valid MM (MiniMax) value under the time constraint (admissible to the optimum under the time constraint).\\
2. We finish a certain level of deepening and have a definite horizon compared to deepening over certain action and not on searching over other possible operations due to time limit. 

In $\alpha\beta$ pruning we have other advantages:\\
1. We save time by not searching unneeded branches. If we have a time limit and in a RB MM, we will utilize time better and may go deeper in our game search.
2. The average time complexity is much better compared to regular MM.

Other changes that can be useful are:\\
1. If we apply both iterative deepening and $\alpha\beta$ pruning, we can reorder the children based on previous iteration of iterative deepening and thus make the the $\alpha\beta$ pruning more effective (most likely that we will prune much more).
2. States table - we can save a hash table with states as keys and check if we were in a specific state before in order to save the MM search from that state. However, in our taxi env, the probability for a repetition of a specific state is close to 0. We can however, use part of the state features to estimate the MM value.
3. Selective deepening - one can use selective deepening. Meaning, in certain cases we will go deeper in minimax search.\\
Such cases can be when we're at a "noisy" state, or when the last move was revolutionary. An idea for measuring it might be by a big differences between heuristic values between consecutive states.\\
We can deepen until the difference is small (calm).

\subsection*{B.3}
In code 

\section*{C - $\alpha\beta$ Agent}

\subsection*{C.1}
In code 

\subsection*{C.2}
In k multiagent game, the MM would have a vector of values at each tree node and each agent will try to maximize its value based on its children values (based on the agent specific corresponding value in the vector).\\
When we had only 2 agent, minimizing the other agent value is like maximizing ours as it is a zero sum game, But, in k agents we don't have this degree of freedom, so we use maximization over each agent value.\\
At the leaf states we check who won for our playing agent and return a low utility function in case we lose compared to other agents values.\\
As we increase K the running time will increase as the strategy game tree  will be deeper. We can use $\beta$ pruning to save paths of maximization (k $\beta$ values)\\

\begin{comment}
The average branching factor (in the ideal case in which we prune all with ideal children ordering) $b'$ will be $\sqrt[k]{b}$ as the recursion formula (as seen on lecture) for the K case is:
$$T(d)= T(d-1) + (b-1)T(d-(k+1))$$
\end{comment}
\subsection*{C.3}
Yes, since we optimize and search less paths in the game tree, we have more time to increase our deepening and thus select more accurate worst case compared to w/o pruning.\\
Thus, our decision should be better (doesn't rely on the heuristic at a shallow level as regular MM)\\

\section*{D - ExpectiMax Agent}

\subsection*{D.1}

\end{document}

