\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\graphicspath{ {./plots/} }

\newcommand{\rectres}[1]{
\begin{center}
\begin{tabular}{ |c| }
\hline\\
#1\\
\\
\hline
\end{tabular}
\end{center}
}

\newcommand{\qed}{\hfill$\blacksquare$}

\title{Introduction to AI - 236501\\HW1}
\author{Yair Nahum 034462796\\and\\Hala 209419134 }

\begin{document}

\maketitle

%\tableofcontents{}

\section{Intro}

\subsection{}
We use the notation as in the tutorial about the sets as follows:\\
$$[n] \equiv \{ 1,2,...,n\}$$
Next, we define the $\{S,O,I,G\}$ as follows:\\
For the states we know we have 25 places for the taxi to be in the grid.\\
We have 5 states for the passenger to be $\{R,Y,G,B\}\cup\{in the taxi\}$.
and 4 states for the destination $\{R,Y,G,B\}$.\\
Thus, 
$$S = [500] \Rightarrow $$
$$ |S| = 25*5*4 = 500$$
The valid states as defined in the hw notebook are a subset of this states space as the passenger and destination are not at the same location (3 options are left to seleect from for the destination). So,\\
$$ |S| = 25*5*3 = 375$$
The operations/actions that our taxi agent can perform are:\\
$$O = \{0="South",1="North",2="West",3="East",4="Pickup",5="Dropoff"\}\Rightarrow$$
$$|O| = 6$$
The initial states can be random, but as defined in the hw notebook, we always starts at $s = 328$. Thus,\\
$$I=\{328\}\Rightarrow |I|=1$$
The goal state is when the taxi and passenger are at the destination after the passenger was dropped there. So we have 4 different goal states:\\

$$G=\{s\in S | \{Taxi\wedge Passenger \wedge Destination\in R\} \cup $$
$$\{Taxi\wedge Passenger \wedge Destination\in Y\} \cup $$
$$\{Taxi\wedge Passenger \wedge Destination\in G\} \cup $$
$$\{Taxi\wedge Passenger \wedge Destination\in B\} \} \Rightarrow$$
$$|G|=4$$
\subsection{}
The Domain of the "North" operation are all the states. There are states in which the taxi will stay at the same state if it goes north.
More formally:
$$Domain(o_1="North")\equiv\{s\in S | o_1(s)\neq \phi\} = S$$
One can think that it's illegal to go north on the top row of the grid world or when  there are horizontal walls north to a reachable grid cell. But the env allows the taxi to go north with reward -1.
More formally, if it was illegal:\\
$$Domain(o_1="North")\equiv\{s\in S | o_1(s)\neq \phi\} = $$
$$\{s\in S |s\in \{\text{All states in which the taxi is not at the north-est grid row} \}\}$$

\subsection{}
The function Succ over the initial state 328 will return all the neighbors of that state, and as we got from running getNeigbours on the initial state\\
$$South(328)=428$$
$$North(328)=228$$
$$East(328)=348$$
On West/Dropof/Pickup operation we stay at the same location (there is a wall to the west):
$$West(328)=Pickup(328)=Dropoff(328)=328$$
More formally:
$$Succ(s=328)\equiv\{s'\in S | \exists o\in O, s.t. [s \in Domain(o)\wedge o(s)=s'\} = $$
$$\{428,228,348,328\}$$
\subsection{}
for the worst case, we can randomly switch between several cases and never reach a destination. so for the random agent we can get infinite number of actions for the agent.\\
Since the state space is finite, and there is an equal probability for each action, the agent eventually will find the goal (finite MDP with positive probability and we can compute the expectation for reaching the goal).
\subsection{}
If the agent does the optimal actions by chance, we can see the optimal path is:\\
1. 4 actions to reach the passenger (North,West,South,South)\\
2. 1 action to Pickup\\
3. 4 North actions\\
4. 1 action to Dropoff\\
Therefore, we have total of 11 actions at the optimal path.
\subsection{}
As we wrote on the previous section, the optimal path is:
(North, North, West, South, South, Pickup, North, North, North, North, Dropoff)
The rewards are therefore:\\
(-1, -1, -1, -1, -1, -1, -1, -1, -1, 20)
And the total reward is:
$$\text{total reward} = 9*(-1) + 20 = 11$$

\subsection{}
Yes. There are circles in our search space (if we don't do some graph search that denotes already visited states).\\
For example the following path will get us back to our initial path:\\
(North,East,South,West)\\
If our algorithms of search will maintain the CLOSED lookup table, we can easily check for such circles.

\section{BFS-G}

\subsection{}
We've implemented the BFS-G in code (not tree search).\\
That is, Besides the frontier (the OPEN queue implemented with python list), we maintain a set of states that were already reached (the CLOSE set as described in the book).\\
We've optimized the number of nodes created when expanding some node in the frontier by checking if its neighbor state was already reached.\\
We've added also some counters for the created and expanded nodes through the search.\\

\subsection{}
The amount of states expanded were 31. If we've used tree search it would have been $b^d=6^10=60,466,176$ , when b is the branching factor (number of possible operations) and d is the shortest path of the goal state from the initial state\\
BTW, the amount of created nodes was 36 (about 5 nodes remained in the frontier)\\

\subsection{}
This is the same as previous section as we've optimized to create only none reached (not in CLOSE set) states.

\subsection{}
The main advantage of the BFS/BFS-G compared to DFS (w/o backtracking) is that it's a complete (guaranteed to find a path to the goal) and optimal (that path is the optimal path to the goal state), while DFS (tree search) isn't as it may stuck in infinite loops repeating the same states.\\

In our taxi env, the state space is finite, so if we used DFS-G with graph search, we would have found the solution but it may not be optimal.\\

The disadvantage of BFS is its memory consumption which is $O(b^d)$ while the DFS-G $O(bD)$ (with backtracking $O(D)$) When D is the max depth of the taxi env when we start from some state (bounded by the state space size). DFS with tree search may not end and have an infinite memory consumption as we can repeat the same state in the tree.

\subsection{}
Yes. In the taxi environment it will always find the optimal solution, as we saw in the lecture,\\

BFS finds the shortest path (number of edges) to any state in the env.\\
In our case, the taxi env, the edges (actions) doesn't have the same cost but we will still find the optimal solution as the shortest path in edges is also the shortest path in cost.
\subsubsection*{Proof}
Completeness: Assuming a solution of cost (depth) d exists. BFS will return the solution before exposing the d+1 layer, We know it will reach the d+1 layer as it works in layers and we can by induction prove that it finds all the nodes in the next layer.
Optimal: Assume that a solution of cost (depth) d exists and the BFS returns a none optimal solution of cost dâ€™ > d. It will return the optimal solution before exposing the d+1 layer so we get a contradiction.





\end{document}

