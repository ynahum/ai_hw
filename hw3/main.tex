\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{comment}

\graphicspath{ {./} }

\newcommand{\rectres}[1]{
\begin{center}
\begin{tabular}{ |c| }
\hline\\
#1\\
\\
\hline
\end{tabular}
\end{center}
}

\newcommand{\qed}{\hfill$\blacksquare$}

\title{Introduction to AI - 236501\\HW3}
\author{Yair Nahum 034462796\\and\\Hala Awwad 209419134 }

\begin{document}

\maketitle

%\tableofcontents{}

\section*{MDP}

\subsection*{A}

\subsubsection*{A.a}

In order to work with our R(s) instead of R(s,a,s') one can define the following:
$$\tilde R(s_t=s) = \mathbb {E}_{s'\in S, a\in A(s)}[R(s_{t+1}=s',a_t=a|s_t=s]=$$
$$= \sum_{s'\in S} \sum_{a\in A(s)} P(s_{t+1}=s',a_t=a|s_t=s) R(s_t=s,a_t=a,s_{t+1}=s') = $$
$$= \sum_{s'\in S} \sum_{a\in A(s)} P(s_{t+1}=s'|a_t=a,s_t=s)\pi(a_t=a|s_t=s) R(s_t=s,a_t=a,s_{t+1}=s') = $$
$$= \sum_{a\in A(s)} \pi(a_t=a|s_t=s) \sum_{s'\in S} P(s_{t+1}=s'|a_t=a,s_t=s) R(s_t=s,a_t=a,s_{t+1}=s') = $$

Thus, the value function on states $s\in S$ with some given policy $\pi(a|s)$ (probability to make an action $a$ when we're in s) is defined recursively by:

$$V^{\pi}(s) = \sum_{a\in A(s)} \pi(a|s) \sum_{s'\in S} P(s'|s,a) [R(s,a,s') + \gamma V(s')]$$

In case we have a deterministic given policy, we can write it as:

$$V^{\pi}(s) = \sum_{s'\in S} P(s'|s,\pi(s)) [R(s,\pi(s),s') + \gamma V(s')]$$

(We assumed a stationary policy and dynamics so we didn't subscript/relate to time on probabilities.)

\subsubsection*{A.b}

The Bellman operator:

$$V^*(s) = \max_{a\in A(s)}Q^*(s,a) = \max_{a\in A(s)} \sum_{s'\in S} P(s'|s,a) [R(s,a,s') + \gamma V^*(s')]$$

\subsubsection*{A.c}

Value Iteration (VI):\\

\begin{enumerate}
  \item Start with any initial value function (zero or random) $V_0(s),\quad \forall s\in S$.
  \item Compute recursively, for $n = 0,1,2, \ldots $ umtil stopping rule is met (see next the stopping rule details:
          \[V_{n + 1}(s) = \max_{a\in A(s)} \sum_{s' \in S} P(s'|s,a)[R(s,a,s') + \gamma V_n(s')], \quad \forall s \in S\]
   \item Extract optimal policy from optimal value by:
   \[\pi(s) \in arg\max_{a\in A(s)} \sum_{s' \in S} P(s'|s,a)[R(s,a,s') + \gamma V^*(s')], \quad \forall s \in S\]
\end{enumerate}
Notes:\\
1. Stopping rule to obtain a required accuracy.\\
  If $$||V_{n+1}-V_{n}||_\infty < \epsilon \cdot \frac{1-\gamma}{2\gamma}$$ then $$||V^{\pi_{n+1}} -
V^{*}||_\infty \leq \epsilon$$\\
(this is a theorem that can be proved as done in tutorials)\\
2. For a fixed policy value iteration, the recursive computation is changed to have $\pi(s)$ instead of $a$
$$V^{\pi}_{n + 1}(s) = \sum_{s' \in S} P(s'|s,\pi(s))[R(s,\pi(s),s') + \gamma V^{\pi}_n(s')], \quad \forall s \in S$$
This can be solved also by linear system of equations solver as these are linear equations (usually when the state space is not too big)\\

When $\gamma=1$ VI is not guaranteed to converge unless there are absorbing/goal states ($s\in S_G$ no action to transition out from it) that we can get to with probability 1 eventually ( This is known as episodic MDPs or Stochastic Shortest Path problems).

\subsubsection*{A.d}

Policy Iteration (PI):\\
PI consists of 2 main parts: Policy evaluation and Policy improvement.

\begin{enumerate}
\item Select some stationary policy $\pi_0$.
\item For $k = 0,1,2, \ldots $ until stopping rule is met (see next the stopping rule details:
\begin{enumerate}
\item Policy Evaluation: compute $V^{\pi_k}$ by fixed policy VI or by solving linear system of equations:\\
$$V^{\pi_k}_{n + 1}(s) = \sum_{s' \in S} P(s'|s,\pi_k(s))[R(s,\pi_k(s),s') + \gamma V^{\pi_k}_n(s')], \quad \forall s \in S$$
Or:\\
$$V^{\pi_k} = (I - \gamma P^{\pi_k})^{-1} P^{\pi_k}R^{\pi_k}$$
(at the last closed solution there is the $R^{\pi_k}$ which corresponds to the vector of rewards $R(s,\pi_k(s),s')$ multiplied by the transition probabilities matrix for expectancy)
\item Policy Improvement: compute $\pi_{k+1}$, a greedy policy with respect to $V^{\pi_k}$:
$$\pi_{k+1}(s) \in arg\max_{a\in A(s)} \sum_{s' \in S} P(s'|s,a)[R(s,a,s') + \gamma V^{\pi_k}(s')], \quad \forall s \in S$$
\item Stop if $\pi_{k+1} = \pi_{k}$
\end{enumerate}
\end{enumerate}

When $\gamma=1$ PI is not guaranteed to converge unless there are absorbing/goal states ($s\in S_G$ no action to transition out from it) that we can get to with probability 1 eventually ( This is known as episodic MDPs or Stochastic Shortest Path problems).

\subsubsection*{A.e}

\includegraphics[]{hw3/plots/A_5.PNG}

\subsubsection*{A.f}

\includegraphics[]{hw3/plots/A_6.PNG}

\end{document}

